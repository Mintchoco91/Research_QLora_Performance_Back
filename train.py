import transformers
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    Trainer, 
    TrainingArguments, 
    BitsAndBytesConfig, 
    EarlyStoppingCallback, 
    default_data_collator,
    DataCollatorWithPadding,
    TrainerCallback
)
import math
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from torch.optim import AdamW
import torch
import wandb
from torch.nn import CrossEntropyLoss
import time

# ============================================================
# âœ… ì‹¤í—˜ ë°ì´í„°
# ============================================================
idx = 5
weight = "1B"
model_id = "meta-llama/Llama-3.2-" + weight
isQLora = True  # â† LoRAë¡œ ë°”ê¾¸ë ¤ë©´ Falseë¡œ
rank_val = 32
alpha_val = 64
Scaling_factor = float(alpha_val/rank_val)


class VRAMLoggingCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB ë‹¨ìœ„)
        real_vram_peak = torch.cuda.memory_allocated() / 1024**2
        reserve_vram_step = torch.cuda.max_memory_reserved() / 1024**2
        wandb.log({"Real_Vram_Step_MB": real_vram_peak, "step": state.global_step})
        wandb.log({"Reserve_Vram_Step_MB": reserve_vram_step, "step": state.global_step})

        


# =========================================
# âœ… Trainable Parameters ê³„ì‚° í•¨ìˆ˜
# =========================================
def print_trainable_parameters(model):
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    percent = 100 * trainable_params / all_params
    print(f"Trainable params: {trainable_params:,}")
    print(f"All params: {all_params:,}")
    print(f"Trainable%: {percent:.4f}%")
    return percent



# ============================================================
# âœ… 1. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================
def tokenize_func(batch):
    texts = []
    for instr, inp, out in zip(batch["instruction"], batch["input"], batch["output"]):
        prompt = f"{instr}\n{inp}\nAnswer: " if inp else f"{instr}\nAnswer: "
        text = prompt + out

        # Dynamic padding
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=512,
            padding="max_length",   # âœ… ê³ ì • íŒ¨ë”©
            add_special_tokens=False
        )

        # Mask prompt tokens (-100)
        labels_ids = tokenized["input_ids"].copy()
        prompt_len = len(tokenizer(prompt, add_special_tokens=False)["input_ids"])
        labels_ids[:prompt_len] = [-100] * prompt_len
        tokenized["labels"] = labels_ids
        texts.append(tokenized)

    # Batchify
    return {k: [dic[k] for dic in texts] for k in texts[0]}

# ============================================================
# âœ… Perplexity
# ============================================================
def evaluate_token_weighted_ppl(trainer, eval_dataset):
    model = trainer.model
    model.eval()
    loss_fct = CrossEntropyLoss(ignore_index=-100, reduction="sum")  # í•©ê³„ë¡œ ëª¨ìŒ
    total_nll = 0.0
    total_tokens = 0

    dataloader = trainer.get_eval_dataloader(eval_dataset)
    device = next(model.parameters()).device

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(input_ids=batch["input_ids"],
                            attention_mask=batch.get("attention_mask", None),
                            labels=batch["labels"])
            # outputs.lossëŠ” ë°°ì¹˜ í‰ê· . ëŒ€ì‹  ë¡œì§“ìœ¼ë¡œ ì§ì ‘ í•©ê³„ NLL ê³„ì‚°
            logits = outputs.logits[:, :-1, :].contiguous()
            labels = batch["labels"][:, 1:].contiguous()  # next-token loss
            # í‰íƒ„í™”
            shift_logits = logits.view(-1, logits.size(-1))
            shift_labels = labels.view(-1)
            nll = loss_fct(shift_logits, shift_labels)  # í•©ê³„ NLL
            valid = (shift_labels != -100).sum()

            total_nll += nll.item()
            total_tokens += valid.item()

    avg_nll = total_nll / max(total_tokens, 1)
    ppl = math.exp(avg_nll)
    return ppl, avg_nll, total_tokens

# ============================================================
# âœ… 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ============================================================
#model_id = "meta-llama/Llama-3.2-1B"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print(f"isQLora Mode : ", str(isQLora))

wandb.init(
    project="llama3-qlora",
    name="idx-" + str(idx) +"-weight-"+ str(weight) + "-rank-" + str(rank_val) + "-alpha-" + str(alpha_val) + "-qlora-" + str(isQLora),
    config={"lr": 5e-5, "epochs": 8}
)

if isQLora:
    # âœ… QLoRA ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )
    optim_type = "adamw_bnb_8bit"

else:
    # âœ… ì¼ë°˜ LoRA ì„¤ì • (ë¹„ì–‘ìí™”)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    optim_type = "adamw_torch"

output_dir_name = "./result/weight-"+ str(weight) + "-rank-" + str(rank_val) + "-alpha-" + str(alpha_val) + "-qlora-" + str(isQLora)


# ============================================================
# âœ… 3. LoRA ì„¤ì •
# ============================================================
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.config.use_cache = False
model.config.pad_token_id = tokenizer.pad_token_id
model.config.eos_token_id = tokenizer.eos_token_id
model.config.bos_token_id = tokenizer.bos_token_id

lora_config = LoraConfig(
    r=rank_val,
    lora_alpha=alpha_val,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

'''backup
lora_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)
'''
model = get_peft_model(model, lora_config)

# 4ï¸âƒ£ íŒŒë¼ë¯¸í„° ê³„ì‚°
trainable_percent = print_trainable_parameters(model)
model.print_trainable_parameters()


# ============================================================
# âœ… 4. ë°ì´í„°ì…‹ ë¡œë“œ ë° í† í¬ë‚˜ì´ì§•
# ============================================================
dataset = load_dataset(
    "json",
    data_files={
        "train": "./train_data/train.jsonl",
        #"train": "./train_data/custom_train.jsonl",
        "test": "./train_data/eval.jsonl"
    }
)

# âœ… ì—¬ê¸°ì„œ ìƒ˜í”Œ ê°œìˆ˜ ì¤„ì´ê¸° (sanity checkìš©)
#dataset["train"] = dataset["train"].select(range(100))   # í›ˆë ¨ ë°ì´í„° 8ê°œë§Œ
#dataset["test"]  = dataset["test"].select(range(2))    # í‰ê°€ ë°ì´í„° 2ê°œë§Œ


tokenized_train = dataset["train"].map(
    tokenize_func,
    batched=True,
    remove_columns=dataset["train"].column_names
)

tokenized_eval = dataset["test"].map(
    tokenize_func,
    batched=True,
    remove_columns=dataset["test"].column_names
)


# ============================================================
# âœ… 5. Trainer ì„¸íŒ…
# ============================================================
training_args = TrainingArguments(
    # âš™ï¸ ê¸°ë³¸ í•™ìŠµ ì„¤ì •
    per_device_train_batch_size = 1,       # 8GB VRAM ì•ˆì „ì„ 
    gradient_accumulation_steps = 4,       # ì‹¤íš¨ ë°°ì¹˜ 4 â†’ ì•ˆì •ì  ìˆ˜ë ´
    num_train_epochs = 2,                  # âœ… 3 â†’ 2ë¡œ ì¤„ì„ (ê³¼ì í•© ë°©ì§€)
    learning_rate = 2e-4,                  # âœ… 2e-4 â†’ 1e-4ë¡œ ì™„í™” (loss ê¸‰ë½ ì–µì œ)

    # ğŸ”„ ìŠ¤ì¼€ì¤„ ë° ìµœì í™”
    warmup_ratio = 0.05,                   # âœ… 0.03 â†’ 0.05ë¡œ ìƒìŠ¹ (ì´ˆë°˜ ì•ˆì •ì„± ê°•í™”)
    lr_scheduler_type = "cosine",          # cosine decayë¡œ ì™„ë§Œí•œ ê°ì†Œ
    weight_decay = 0.05,                   # âœ… 0.01 â†’ 0.05ë¡œ ì¦ê°€ (ì¼ë°˜í™” ê°•í™”)
    max_grad_norm = 0.8,                   # âœ… ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼ ë°©ì§€ ê°•í™”
    optim = optim_type,                    # LoRA: adamw_torch / QLoRA: adamw_bnb_8bit

    # ğŸ§© ì •ë°€ë„ ì„¤ì •
    fp16 = False,                          # âœ… QLoRA overflow ë°©ì§€
    bf16 = False,                          # RTX 30 ì‹œë¦¬ì¦ˆ ë¯¸ì§€ì›

    # ğŸ§¾ ë¡œê¹… & ì €ì¥
    logging_steps = 20,
    save_strategy = "epoch",               # ì—í­ ë‹¨ìœ„ ì €ì¥
    eval_strategy = "epoch",               # ì—í­ ë‹¨ìœ„ í‰ê°€
    save_total_limit = 3,                  # ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ ìœ ì§€
    load_best_model_at_end = True,         # ê²€ì¦ ì†ì‹¤ ê¸°ì¤€ ë³µì›
    logging_first_step = True,             # ì²« stepë¶€í„° ë¡œê¹…

    # ğŸª£ ì¶œë ¥ ë° ê´€ë¦¬
    output_dir = output_dir_name,
    run_name = "llama3-finetune-compare",
    report_to = "wandb",
)

optimizer = AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=training_args.learning_rate
)

# âœ… collator ë³€ê²½ (labels ìœ ì§€)
data_collator = default_data_collator

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    data_collator=data_collator,
    optimizers=(optimizer, None),
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), VRAMLoggingCallback()]
)


# ============================================================
# âœ… 6. í•™ìŠµ ì‹¤í–‰
# ============================================================
start_time = time.time()    #ì†Œìš”ì‹œê°„ ì¸¡ì • ì‹œì‘
trainer.train()
end_time = time.time()      #ì†Œìš”ì‹œê°„ ì¸¡ì • ë

elapsed = end_time - start_time
hours = int(elapsed // 3600)
minutes = int((elapsed % 3600) // 60)
seconds = int(elapsed % 60)
time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"

wandb.log({
    "train_time_hms": time_str,  # ë³´ê¸° ì¢‹ì€ HH:MM:SS í˜•ì‹
    "train_time_hours": elapsed / 3600,  # ê·¸ë˜í”„ìš© ìˆ«ìí˜• ë°ì´í„°
    "train_time_minutes": elapsed / 60   # ë¹„êµìš©
})

metrics = trainer.evaluate()
ppl = math.exp(metrics["eval_loss"])
print(metrics)


# ğŸ”¹ ì—¬ê¸°ì„œ ì •í™•í•œ PPL ê³„ì‚°
token_weighted_ppl, avg_nll, ntoks = evaluate_token_weighted_ppl(trainer, tokenized_eval)
wandb.log({
    "Perplexity_token_weighted": token_weighted_ppl,
    "Eval_AvgNLL": avg_nll,
    "Eval_Tokens": ntoks
})
print(f"Token-weighted PPL: {token_weighted_ppl:.3f} (avg NLL: {avg_nll:.5f}, tokens: {ntoks})")

# Peak VRAM ê¸°ë¡
real_vram_peak = torch.cuda.memory_allocated() / 1024**2
reserve_vram_peak = torch.cuda.max_memory_reserved() / 1024**2
wandb.log({"Real_VRAM_Peak_MB": real_vram_peak})
wandb.log({"Reserve_VRAM_Peak_MB": reserve_vram_peak})

# ============================================================
# âœ… 7. ëª¨ë¸ ì €ì¥
# ============================================================
trainer.model.save_pretrained("./llama-3.2-1b-lora")
tokenizer.save_pretrained("./llama-3.2-1b-lora")

print("Last checkpoint:", trainer.state.best_model_checkpoint)
print("Output directory:", training_args.output_dir)

# ============================================================
# âœ… 8. ì‹¤í—˜ ê²°ê³¼
# ============================================================

print("============================================================")
print(f"isQLora : {isQLora}")
print(f"rank_val : {rank_val}")
print(f"alpha_val : {alpha_val}")
print(f"Scaling_factor : {Scaling_factor}")
print(f"Trainable Params(%) : {print_trainable_parameters(model)}")


print(f"Training Time : {time_str}")
print(f"Perplexity: {token_weighted_ppl}")
print(f"Real_VRAM_Peak_MB: {real_vram_peak:.2f} MB")
print(f"Reserve_VRAM_Peak_MB: {reserve_vram_peak:.2f} MB")
print("============================================================")