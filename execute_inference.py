import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# ============================================================
# âš™ï¸ ì„¤ì •
# ============================================================
isQLora = True   # âœ… Falseë¡œ ë°”ê¾¸ë©´ LoRA ì¶”ë¡ 
base_model_id = "meta-llama/Llama-3.2-3B"
adapter_path = "./trained-llama3-qlora/checkpoint-1000" if isQLora else "./trained-llama3-lora/checkpoint-1000"

# ============================================================
# âœ… 1. ëª¨ë¸ ë¡œë“œ
# ============================================================
if isQLora:
    print("ğŸ”¹ Loading QLoRA 4bit model...")
    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16
    )

    base = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=bnb,
        device_map="auto"
    )

else:
    print("ğŸ”¹ Loading LoRA FP16 model...")
    base = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )

# ğŸš« merge_and_unload() ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (QLoRAì¼ ë•Œ rounding ë¬¸ì œ)
model = PeftModel.from_pretrained(base, adapter_path)
model.eval()

# ============================================================
# âœ… 2. í† í¬ë‚˜ì´ì €
# ============================================================
tok = AutoTokenizer.from_pretrained(base_model_id)
tok.pad_token = tok.eos_token
tok.padding_side = "right"

# ============================================================
# âœ… 3. í”„ë¡¬í”„íŠ¸
# ============================================================
prompt = "Answer like Thrall.\nTell me about who you are.\nAnswer: "

inputs = tok(prompt, return_tensors="pt").to(model.device)

# ============================================================
# âœ… 4. ìƒì„±
# ============================================================
with torch.inference_mode():
    out = model.generate(
        **inputs,
        max_new_tokens=60,
        min_new_tokens=10,   # âœ… ìµœì†Œ ê¸¸ì´ í™•ë³´
        do_sample=True,
        temperature=0.8,
        top_p=0.9,
        repetition_penalty=1.2,
        no_repeat_ngram_size=3
    )
'''
with torch.inference_mode():
    out = model.generate(
        **inputs,
        max_new_tokens=40,
        do_sample=True,
        temperature=0.8,
        top_p=0.9,
        repetition_penalty=1.2,
        no_repeat_ngram_size=3
    )
'''
decoded = tok.decode(out[0], skip_special_tokens=True)

print("===============================================")
print(f"ğŸ¦™ Mode     : {'QLoRA (4bit)' if isQLora else 'LoRA (FP16)'}")
print("prompt : ", prompt)
print("answer : ", decoded[len(prompt):].strip())
print("===============================================")
